{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cdab39-c29a-49dc-a49a-2712bc05ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "eps = 1e-10\n",
    "\n",
    "\n",
    "def cosine_similarity(previous_action, action):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors.\n",
    "\n",
    "    Parameters:\n",
    "    previous_action (np.ndarray): The first vector.\n",
    "    action (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    previous_action = previous_action.reshape(-1)\n",
    "    action = action.reshape(-1)\n",
    "    \n",
    "    norm_previous_action = np.linalg.norm(previous_action)\n",
    "    norm_action = np.linalg.norm(action)\n",
    "    \n",
    "    # Handle the case where one of the norms is zero\n",
    "    if norm_previous_action == 0 or norm_action == 0:\n",
    "        return 0.0  # or np.nan or raise an exception depending on your needs\n",
    "    \n",
    "    cosine = np.dot(previous_action, action) / (norm_previous_action * norm_action)\n",
    "    \n",
    "    # Check for NaN value\n",
    "    if np.isnan(cosine):\n",
    "        cosine = 0.0\n",
    "    \n",
    "    return cosine\n",
    "\n",
    "\n",
    "def sharpe(returns, freq=252, rf=0):\n",
    "    return (np.sqrt(freq) * np.mean(returns - rf + eps)) / np.std(returns - rf + eps)\n",
    "\n",
    "\n",
    "def max_drawdown(returns):\n",
    "    \"\"\" Max drawdown \"\"\"\n",
    "    log_r = np.log(1 + returns)\n",
    "    log_cum_r = np.cumsum(log_r)\n",
    "    r_box = log_cum_r.copy()\n",
    "    for i in range(len(returns)):\n",
    "        r_box[i] = log_cum_r[i] - np.max(log_cum_r[0:i])\n",
    "    MD = 1 - np.exp(np.min(r_box))\n",
    "    \n",
    "    return MD\n",
    "\n",
    "def dataframe_to_numpy(data, index_name, shape):\n",
    "    '''\n",
    "    change multiindex dataframe to numpy array\n",
    "    \n",
    "    '''\n",
    "    a = data[index_name[0]].to_numpy().reshape(shape)\n",
    "    a = a.astype(np.float32)\n",
    "    for i in index_name[1:]:\n",
    "        b = data[i].to_numpy().reshape(shape)\n",
    "        b = b.astype(np.float32)\n",
    "        a = np.append(a,b, axis = 0)\n",
    "    return a\n",
    "\n",
    "def update_weight(w0, r0):\n",
    "    if sum(r0 * w0) != 0:\n",
    "        dw0 = (r0 * w0) / sum(r0 * w0)\n",
    "    else:\n",
    "        dw0 = w0 * 0 # keep the size \n",
    "    return dw0\n",
    "        \n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"Acts as data provider for each new episode.\"\"\"\n",
    "\n",
    "    def __init__(self, history, abbreviation, steps = 200, window_length = 5, eps_move = 10, start_date = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            history:  MultiIndex pandas DataFrame with shape \n",
    "            (his_window, num_stocks * feature) \n",
    "                feature: open, high, low, close, volume\n",
    "            abbreviation: stock name\n",
    "            steps: the total number of steps to simulate, default is 200 days\n",
    "            window_length: observation window\n",
    "            eps_move: move the start date at each reset in roll\n",
    "            start_date: Start Date\n",
    "        \"\"\"\n",
    "        import copy\n",
    "        \n",
    "        \n",
    "        self.reset_pointer = 0\n",
    "        \n",
    "        self.step = 0\n",
    "        self.steps = steps \n",
    "        self.window_length = window_length\n",
    "        \n",
    "        \n",
    "        self.num_stock = len(abbreviation)\n",
    "        self.num_feature = int(history.shape[1]/self.num_stock)\n",
    "        \n",
    "        self.eps_move = eps_move\n",
    "        self.start_date = datetime.strptime(start_date, '%Y-%m-%d' ) # e.g., \"2017-10-22\"\n",
    "\n",
    "        # make immutable class\n",
    "        self._data = history#.copy()  # all data\n",
    "        self.asset_names = copy.copy(abbreviation)\n",
    "\n",
    "    def _step(self):\n",
    "        # get observation matrix from history\n",
    "\n",
    "        self.step += 1\n",
    "        \n",
    "        obs = self.data[:, self.step:self.step + self.window_length, :].copy()\n",
    "        obs = obs.reshape(1, self.num_stock, self.window_length, self.num_feature)\n",
    "        \n",
    "        obs = obs.astype(np.float32)\n",
    "\n",
    "\n",
    "        # used for compute optimal action and sanity check\n",
    "        ground_truth_obs = self.data[:, self.step + self.window_length:self.step + self.window_length + 1, :].copy()\n",
    "\n",
    "        done = self.step  >= self.steps \n",
    "        return obs, done, ground_truth_obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "        self.reset_pointer += 1\n",
    "\n",
    "        # get data for this episode, each episode might be different.\n",
    "        if self.start_date is None:\n",
    "            self.idx = np.random.randint(\n",
    "                low=self.window_length, high=self._data.shape[1] - self.steps)\n",
    "        else:\n",
    "            # compute index corresponding to start_date for repeatable sequence\n",
    "            self.idx = self.start_date + timedelta(days=(self.reset_pointer - 1) * self.eps_move)\n",
    "            \n",
    "            \n",
    "        # data start with self.idx - self.window_length\n",
    "        # find  start date - window size \n",
    "        \n",
    "        start_date_windows = self._data.loc[:self.idx].copy() \n",
    "        # all the history before the start date (include the start date, by loc method)\n",
    "        start_date_windows = start_date_windows.iloc[(-self.window_length ),:] # so that start date \n",
    "        start_date_windows = str(start_date_windows.name)[0:10] # start date - window size, \n",
    "        # we cant just let date - timedelta(days = window size) since the weekend and holiday are not count in data but count in timedelta\n",
    "        \n",
    "        data = self._data.loc[start_date_windows:].copy()\n",
    "        data = data.iloc[0:(self.window_length + self.steps) , :] # +2 for true_growth\n",
    "        assert data.shape[0] > 0, \\\n",
    "                'Invalid start date, must be window_length day after start date and simulation steps day before end date'\n",
    "        \n",
    "        # transform the data to numpy array with shape (m_stock, his, features)\n",
    "        data = dataframe_to_numpy(data, self.asset_names, (1, data.shape[0], self.num_feature))\n",
    "        self.data = data\n",
    "        \n",
    "        #  first obs\n",
    "        obs = data[:, self.step:self.step + self.window_length, :]\n",
    "        obs = obs.reshape(1, self.num_stock, self.window_length, self.num_feature)\n",
    "        obs = obs.astype(np.float32)\n",
    "        \n",
    "        return obs, \\\n",
    "               self.data[:, self.step + self.window_length:self.step + self.window_length + 1, :].copy()\n",
    "    \n",
    "    \n",
    "def best_performance_stock(y):\n",
    "    '''\n",
    "    y have form (1.1 , 1.2, 0.9, ...) which is the past return\n",
    "    '''\n",
    "    loc = np.argmax(y)\n",
    "    w = np.zeros(len(y))\n",
    "    w[loc] = 1\n",
    "    return w\n",
    "\n",
    "def performance_rank(y):\n",
    "    '''\n",
    "    return the rank of return, e.g. [1.1,1.3,0.9] -> [1,2,0] best performance give highest value \n",
    "    \n",
    "    '''\n",
    "    x = y.argsort()\n",
    "    ranks = np.empty_like(x)\n",
    "    ranks[x] = np.arange(len(y))\n",
    "    return ranks\n",
    "\n",
    "\n",
    "class PortfolioSim(object):\n",
    "    \"\"\"\n",
    "    Compute the reward and record the step\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, asset_names=list(), steps=200, trading_cost=0.005, time_cost=0.0, alpha = 0.05, \\\n",
    "                 beta = 0.05 ,gamma_ = 0.01):\n",
    "        self.asset_names = asset_names\n",
    "        self.cost = trading_cost\n",
    "        self.time_cost = time_cost\n",
    "        self.steps = steps\n",
    "        self.reset()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma_ = gamma_\n",
    "\n",
    "    def _step(self, w1, w0, observation, ground_truth_obs):\n",
    "        \"\"\"\n",
    "        Used to compute rewards based on given action and observation\n",
    "        \n",
    "        Args:\n",
    "            w1 - new action of portfolio weights - e.g. [[0.1,0.9,0.0]] coz its output of network\n",
    "            w0 - previous action \n",
    "            y0 - previous price relative vector, also called return\n",
    "                e.g. [1.0, 0.9, 1.1]\n",
    "            observation used to compute reward, has shape (1, m_stock, his_window, features)\n",
    "            beta: for variance\n",
    "            gamma: for max weight\n",
    "            \n",
    "        w0 = 0 for the initial states\n",
    "        w1 will be the first weight\n",
    "        \n",
    "        \"\"\"\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        gamma_ = self.gamma_\n",
    "      \n",
    "        w1 = w1[0] # e.g., [[0.1,0.9,0.0]]\n",
    "        w0 = w0[0]\n",
    "        \n",
    "        # assert sum(w1) != 1.0, 'weight sum are not equal to 1'\n",
    "        #if sum(w1) != 1.0:\n",
    "            #print(sum(w1))\n",
    "        \n",
    "        num_stock = len(w1)\n",
    "\n",
    "        p0 = self.p0 # portfolio value\n",
    "        \n",
    "        if sum(w0) == 0: # initial step\n",
    "            dw0 = w0\n",
    "            previous_return = 1\n",
    "            y0 = np.array([1] * num_stock)\n",
    "            \n",
    "            # equal weights portfolio \n",
    "            equal_hold_weight = np.array([0] * num_stock)\n",
    "            update_equal_hold_weight = np.array([1/num_stock] * num_stock)\n",
    "            eq_r = 1\n",
    "            \n",
    "            # best performance stocks -> non mi serve piu: replace with benchmark performance (penso possa essere aggiunta anche fuori)\n",
    "            window_past_return = np.array(observation[0][:, -2, 3]/observation[0][:, 0, 3])\n",
    "            bp_weight = np.array([0] * num_stock)\n",
    "            update_bp_weight = best_performance_stock(window_past_return)\n",
    "            self.bp_weight = bp_weight\n",
    "            bp_r = 1\n",
    "            \n",
    "            self.weighted_rank = 4.5 # che cos'Ã¨?\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            close_price_vector = observation[0][:, -1, 3] # obs has shape (1, m_stock, window, feature)\n",
    "            previous_close_price_vector = observation[0][:, -2, 3]\n",
    "            y0 = np.array(close_price_vector / previous_close_price_vector)\n",
    "            previous_return = np.dot(y0, w0) # portfolio returns\n",
    "            dw0 = update_weight(w0, y0)  # update past weight\n",
    "            \n",
    "            \n",
    "            # equal weights portfolio \n",
    "            equal_hold_weight = np.array([1/num_stock] * num_stock)\n",
    "            update_equal_hold_weight = update_weight(equal_hold_weight, y0)\n",
    "            eq_r = np.dot(equal_hold_weight, y0) # period return \n",
    "            \n",
    "            \n",
    "            # best performance stocks -> non mi serve piu: replace with benchmark performance (penso possa essere aggiunta anche fuori)\n",
    "            x = observation[0][:, -2, 3]/observation[0][:, 0, 3]\n",
    "            window_past_return = np.array(x)\n",
    "            update_bp_weight = best_performance_stock(window_past_return)\n",
    "            bp_r = np.dot(update_bp_weight, y0)\n",
    "            \n",
    "            # track the weighted rank \n",
    "            #x_ = ground_truth_obs[0][:, 3]/observation[0][:, -1, 3]\n",
    "            self.weighted_rank = np.dot(performance_rank(y0) , w0)\n",
    "            \n",
    "        \n",
    "        # equal weights portfolio \n",
    "        equal_hold_weight_cost = self.cost * np.sum(np.abs(equal_hold_weight - update_equal_hold_weight))\n",
    "        eq_r = eq_r * (1 - equal_hold_weight_cost) # add transaction cost \n",
    "        eq_r = eq_r * (1 - self.time_cost)\n",
    "        self.eq_p0 = self.eq_p0 * eq_r\n",
    "        \n",
    "        \n",
    "        \n",
    "        # best past performance stocks, weight has form [0,0,1,....,0]\n",
    "        bp_weight_cost = self.cost * np.sum(np.abs(update_bp_weight - self.bp_weight))\n",
    "        bp_r = bp_r * (1 - bp_weight_cost) # add transaction cost \n",
    "        bp_r = bp_r * (1 - self.time_cost)\n",
    "        self.bp_p0 = self.bp_p0 * bp_r\n",
    "        self.bp_weight = update_bp_weight # update the weight\n",
    "        \n",
    "        # network portfolio \n",
    "        mu1 = self.cost * np.sum((np.abs(w1 - dw0))) \n",
    "        if mu1 != mu1: mu1 = 0\n",
    "        assert mu1 < 1.0, f'trading cost is too large: {mu1}'\n",
    "        p1 = p0 * (1 - mu1) * previous_return  # update final portfolio value\n",
    "        rho1 = p1/p0 - 1\n",
    "        r1 = np.log(1 + rho1)  # log rate of return\n",
    "        # r1 = rho1\n",
    "        \n",
    "        # predicted variance of portfolio\n",
    "        z = np.cov(observation[0][:,:,3], rowvar = True)\n",
    "        predicted_var = np.dot(np.matmul(w1 ,z), w1)\n",
    "        \n",
    "        # max of weight\n",
    "        log_max_w1 =  max(w1)\n",
    "        \n",
    "        # log eqr\n",
    "        log_eq_r = np.log(eq_r)\n",
    "        \n",
    "        # reward = returns - alpha * returns_ewp - beta * predicted_var - gamma * max(action) \n",
    "        #reward = (r1  - alpha *  log_eq_r - beta * predicted_var - gamma_ * log_max_w1) # change according with the proposed reward function\n",
    "        reward = r1\n",
    "        # remember for next step\n",
    "        self.p0 = p1\n",
    "\n",
    "        # if we run out of money, we're done (losing all the money)\n",
    "        done = (p1 <= 0)\n",
    "\n",
    "        info = {\n",
    "            \"reward\": reward,\n",
    "            \"log_return\": r1,\n",
    "            \"portfolio_value\": p1,\n",
    "            \"average_return\": np.mean(y0),\n",
    "            \"rate_of_return\": rho1,\n",
    "            \"weights_std\": np.std(w1),\n",
    "            \"cost\": mu1,\n",
    "            'equal_weight_portfolio_value': self.eq_p0,\n",
    "            'MOM_portfolio_value': self.bp_p0,\n",
    "            'portfolio_rank_weight': self.weighted_rank\n",
    "        }\n",
    "        self.infos.append(info)\n",
    "        return reward, info, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.infos = []\n",
    "        self.p0 = 1.0\n",
    "        self.eq_p0 = 1.0 # equal weight prortfolio value\n",
    "        self.bp_p0 = 1.0 # track best past stock given time window\n",
    "        self.weighted_rank = 4.5 # initial weighted rank, equal to equal weight portfolio\n",
    "        \n",
    "def observation_normalized(observation, num_stock, window_length):\n",
    "    # normalize the open, high, low, close by divided the last close\n",
    "    d1 = observation[:,:,:,0:4]/(observation[:,:,-1,3].reshape(1,num_stock,1,1)+1e-8) # \n",
    "    # normalize the vol\n",
    "    s = observation.shape[-1]-4\n",
    "    d2 = observation[:,:,:,4:]/(observation[:,:,-1,4:].reshape(num_stock,1,s) +1e-8)\n",
    "    d2 = d2.reshape((1,num_stock,window_length,s))\n",
    "    d = np.concatenate([d1,d2],axis = 3)\n",
    "    return d\n",
    "    \n",
    "    \n",
    "def entropy(action):\n",
    "    return -(np.log2(action + 1e-8)*action).sum()/2\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Rl environment for PM\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self,\n",
    "                 history,\n",
    "                 abbreviation,\n",
    "                 steps = 200,  # 2 years\n",
    "                 trading_cost = 0.005,\n",
    "                 time_cost = 0.0,\n",
    "                 window_length = 5,\n",
    "                 eps_move = 10,\n",
    "                 sample_start_date=None,\n",
    "                 alpha = 0, \n",
    "                 beta = 0,\n",
    "                 gamma_ = 0.01,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        An environment for financial portfolio management.\n",
    "        Params:\n",
    "            steps - steps in episode\n",
    "            trading_cost \n",
    "            window_length - length of past observations \n",
    "            eps_move - move the start date ar each rest\n",
    "            sample_start_date - start date \n",
    "        \"\"\"\n",
    "        self.window_length = window_length\n",
    "        self.num_stocks = len(abbreviation)\n",
    "        self.trading_cost = trading_cost\n",
    "\n",
    "        self.src = DataGenerator(history, abbreviation, steps=steps, window_length=window_length, eps_move=eps_move,\n",
    "                                 start_date=sample_start_date)\n",
    "        \n",
    "        self.date = history\n",
    "        self.sim = PortfolioSim(\n",
    "            asset_names=abbreviation,\n",
    "            trading_cost=trading_cost,\n",
    "            time_cost=time_cost,\n",
    "            steps=steps,\n",
    "            alpha = alpha, \n",
    "            beta = beta,\n",
    "            gamma_ = gamma_\n",
    "            )\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        # store the previous action\n",
    "        self.previous_action = np.array([0] * len(self.src.asset_names)).reshape(1 , len(self.src.asset_names))\n",
    "        self.previous_action = self.previous_action.astype(np.float32) \n",
    "\n",
    "        # openai gym attributes\n",
    "        # action will be the portfolio weights from 0 to 1 for each asset\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            0, 1, shape=(1, len(self.src.asset_names)), dtype=np.float32)  # exclude cash\n",
    "\n",
    "        # get the observation space from the data min and max\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(1, len(abbreviation), window_length,\n",
    "                                                                                 history.shape[-1]), dtype=np.float32)\n",
    "        # self.observation_data = np.memmap('observation_data.dat', dtype='float32', mode='w+', shape=(1, len(abbreviation), window_length, history.shape[-1]))\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        return self._step(action)\n",
    "    \n",
    "\n",
    "    def _step(self, action):\n",
    "        \"\"\"\n",
    "        Step the env.\n",
    "        Actions should be portfolio [[w0...]]\n",
    "        - Where wn is a portfolio weight from 0 to 1. The first is cash_bias\n",
    "        - cn is the portfolio conversion weights see PortioSim._step for description\n",
    "        \"\"\"\n",
    "        action = action[:,1:]\n",
    "        reward, info, done2 = self.sim._step(action, self.previous_action,\\\n",
    "                                             self.previous_observation, self.previous_ground_truth_obs ) # compute the reward\n",
    "        ent = entropy(action)\n",
    "\n",
    "        # cosine = np.dot(self.previous_action.reshape(-1), action.reshape(-1))/(np.linalg.norm(self.previous_action.reshape(-1))*np.linalg.norm(action.reshape(-1)))\n",
    "        # if cosine != cosine: cosine = 0\n",
    "\n",
    "        cosine = cosine_similarity(self.previous_action.reshape(-1), action.reshape(-1))\n",
    "        \n",
    "        reward += self.alpha*ent + self.beta*cosine\n",
    "        \n",
    "        # add dates\n",
    "        info['date'] = self.date_track[self.src.step] #self.start_idx + timedelta(days = self.src.step)\n",
    "        # current step\n",
    "        info['steps'] = self.src.step\n",
    "        info['next_obs'] = self.previous_ground_truth_obs\n",
    "        \n",
    "        observation, done1, ground_truth_obs = self.src._step() # move 1 step \n",
    "        self.previous_observation = observation # update for next round\n",
    "        self.previous_ground_truth_obs = ground_truth_obs\n",
    "        \n",
    "\n",
    " \n",
    "        \n",
    "        # normalized the data up to the last close(for open, high, low, close) and last volume (only for vol)\n",
    "        obs_norm = observation_normalized(observation, self.num_stocks, self.window_length)\n",
    "        # obs_norm = observation\n",
    "        \n",
    "        # update the information and action\n",
    "        self.infos.append(info)   \n",
    "        self.previous_action = action \n",
    "\n",
    "        return obs_norm, reward, done1 or done2, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.infos = []\n",
    "        self.sim.reset()\n",
    "        observation, ground_truth_obs = self.src.reset()\n",
    "        self.start_idx = self.src.idx\n",
    "        self.date_track = self.date.loc[self.start_idx:].index # track the true date\n",
    "        self.previous_observation = observation # compute the reward, no need for norm\n",
    "        self.previous_ground_truth_obs = ground_truth_obs\n",
    "        obs_norm = observation_normalized(observation, self.num_stocks, self.window_length)\n",
    "        # obs_norm = observation\n",
    "        # reset the previous action\n",
    "        self.previous_action = np.array([0] * len(self.src.asset_names)).reshape(1 , len(self.src.asset_names))\n",
    "        self.previous_action = self.previous_action.astype(np.float32) \n",
    "        \n",
    "        info = {}\n",
    "        info['next_obs'] = ground_truth_obs\n",
    "        return obs_norm, info\n",
    "\n",
    "    def _render(self, mode='human', close=False):\n",
    "        if close:\n",
    "            return\n",
    "        if mode == 'ansi':\n",
    "            pprint(self.infos[-1])\n",
    "        elif mode == 'human':\n",
    "            self.plot()\n",
    "            \n",
    "    def render(self, mode='human', close=False):\n",
    "        return self._render(mode='human', close=False)\n",
    "\n",
    "    def plot(self):\n",
    "        # show a plot of portfolio, equal weighted portfolio, and simple MOM\n",
    "        df_info = pd.DataFrame(self.infos)\n",
    "        # print(df_info)\n",
    "        df_info['date'] = pd.to_datetime(df_info['date'], format='%Y-%m-%d')\n",
    "        df_info.set_index('date', inplace=True)\n",
    "        mdd = max_drawdown(df_info.rate_of_return)\n",
    "        sharpe_ratio = sharpe(df_info.rate_of_return)\n",
    "        ret = (df_info.portfolio_value.iloc[-1] - df_info.portfolio_value.iloc[0])/df_info.portfolio_value.iloc[0]\n",
    "        title = 'max_drawdown={: 2.2%} sharpe_ratio={: 2.4f} ret={: 2.4f}'.format(mdd, sharpe_ratio, ret)\n",
    "        df_info[[\"equal_weight_portfolio_value\", \"portfolio_value\"]].plot(title=title, fig=plt.gcf(), rot=30)\n",
    "    \n",
    "    def table(self):\n",
    "        # show a plot of portfolio, equal weighted portfolio, and simple MOM\n",
    "        df_info = pd.DataFrame(self.infos)\n",
    "        # print(df_info)\n",
    "        df_info['date'] = pd.to_datetime(df_info['date'], format='%Y-%m-%d')\n",
    "        df_info.set_index('date', inplace=True)\n",
    "        \n",
    "        # RL portfolio\n",
    "        mdd = max_drawdown(df_info.rate_of_return)\n",
    "        sharpe_ratio = sharpe(df_info.rate_of_return)\n",
    "        win_pec= len(df_info.rate_of_return[df_info.rate_of_return > 0]) / len(df_info.rate_of_return)\n",
    "        annual_return = df_info.portfolio_value[-1] ** (252/len(df_info.portfolio_value))\n",
    "        print(f\" Sharpe Ratio = {sharpe_ratio}\")\n",
    "        print(f\" MDD = {mdd}\")\n",
    "        print(f\" Winning percentage = {win_pec}\")\n",
    "        print(f\" Annual Return = {annual_return}\")\n",
    "        \n",
    "        return [sharpe_ratio, mdd, win_pec, annual_return]\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
