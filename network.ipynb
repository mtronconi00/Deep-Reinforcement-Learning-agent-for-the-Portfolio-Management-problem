{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9928de-3647-49a8-b6c3-abb48dfe5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Actor\n",
    "\n",
    "function 1, make decision: \n",
    "\n",
    "    input (s_t), output(action_t)\n",
    "\n",
    "function 2, update decisition model:\n",
    "\n",
    "    input (learning_rate, G_t), output(new model)\n",
    "    \n",
    "function 3, update the tagrget actor network\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorlayer.layers import (BatchNorm, Conv2d, Dropout , Reshape, Dense, Flatten, Input, LocalResponseNorm, MaxPool2d, Concat, Lambda)\n",
    "from tensorlayer.models import Model\n",
    "import tensorlayer as tl\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "\n",
    "# ===========================\n",
    "#   Actor\n",
    "# ===========================\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "class Actor_NetWork(object):\n",
    "    '''\n",
    "    tf_session: tensorflow session\n",
    "    state_shape: shape of state\n",
    "    action_shape: \n",
    "    learning_rate: learning rate of actor\n",
    "    target_lr : learning rate of target actor network\n",
    "    batch size: size of mini batch, used to training\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, gamma, state_shape, action_shape, a_learning_rate, clustering=None, layers=None):\n",
    "        \n",
    "        self.gamma = gamma # discount_factor \n",
    "        \n",
    "        self.state_shape = state_shape # should be [None, m_stock, historic_window, feature]\n",
    "        self.action_shape = action_shape # should be [None, m_stock]\n",
    "        \n",
    "        # Acotr Network\n",
    "        self.actor_learning_rate = a_learning_rate\n",
    "        self.actor_network = self.get_cnn_actor_model(self.state_shape ,\"Actor_Network\", layers=layers) # tensorlayer model\n",
    "        self.actor_network.train()\n",
    "        self.actor_opt = tf.optimizers.Adam(self.actor_learning_rate)\n",
    "        print(self.actor_network)\n",
    "        self.clustering=clustering\n",
    "  \n",
    "\n",
    "        \n",
    "  \n",
    "    \n",
    "    def Generate_action(self, states, prev_action=None, greedy = False):\n",
    "        '''\n",
    "        states shape should be [m_stock, historic_window, feature]\n",
    "        greedy is used to determine random explore \n",
    "        \n",
    "        '''\n",
    "        one = np.ones((states.shape[0], 1)).astype(np.float32)\n",
    "        \n",
    "        if prev_action is None:\n",
    "            prev_action = np.zeros((states.shape[0], self.action_shape[-1])).astype(np.float32)\n",
    "\n",
    "        epsilon = 0.3\n",
    "        \n",
    "        if greedy:\n",
    "            new_action = self.actor_network([states,prev_action[:,1:], one]).numpy()\n",
    "        else:\n",
    "            if np.random.rand() < epsilon:\n",
    "                new_action = np.random.normal(0, 1, np.shape(prev_action))\n",
    "                new_action = np.clip(new_action, 0 ,1) # values outside the interval are clipped to the interval edges\n",
    "                new_action = new_action/np.sum(new_action)\n",
    "                new_action = np.array(new_action).astype(np.float32)\n",
    "            else:\n",
    "                new_action = self.actor_network([states,prev_action[:,1:], one]).numpy()\n",
    "                    \n",
    "        action = new_action\n",
    "        \n",
    "        \n",
    "        if self.clustering is not None:\n",
    "            d = {} \n",
    "            \n",
    "            for i, c in enumerate(self.clustering):\n",
    "                if c not in d:\n",
    "                    d[c] = []\n",
    "                d[c].append((action[0, i], i))\n",
    "                    \n",
    "            positions = []\n",
    "            values = []\n",
    "            for c in d.keys():\n",
    "                value, pos = max(d[c])\n",
    "                positions.append(pos)\n",
    "                values.append(value)\n",
    "    \n",
    "            values = softmax(np.array(values))\n",
    "            new_action = np.zeros_like(action)\n",
    "            new_action[0, positions] = values\n",
    "            action = new_action\n",
    "        \n",
    "        return action \n",
    "    \n",
    "    def learn(self, states, Gt, prev_action=None):\n",
    "        '''\n",
    "        inputs: (states_t,actions_t,rewards_t,states_t+1)\n",
    "        inputs shape: [[batch_size, m_stock, historic_window, feature], [batch_size, actions] \\\n",
    "            ,[batch_size], [[batch_size, m_stock, historic_window, feature]]\n",
    "            \n",
    "        used to update network\n",
    "        \n",
    "        '''\n",
    "        if prev_action is None:\n",
    "            prev_action = np.zeros((states.shape[0], self.action_shape[-1])).astype(np.float32)\n",
    "        \n",
    "        #G_t must be discount to time 0\n",
    "        G_t = tf.constant(Gt, dtype=tf.float32)\n",
    "\n",
    "        one = np.ones((states.shape[0], 1)).astype(np.float32)\n",
    "        \n",
    "        # actor gradients - Monte Carlo Policy Gradient\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(self.actor_network.trainable_weights)\n",
    "            actions = - G_t * tf.math.log(self.actor_network([states,prev_action[:,1:], one])+1e-8) # theta <- theta + b * Gt * gradient\n",
    "            actions = tf.math.reduce_mean(tf.math.reduce_sum(actions, -1))\n",
    "\n",
    "        actor_grads =  tape.gradient(actions, self.actor_network.trainable_weights)\n",
    "        \n",
    "        #print(actor_grads)\n",
    "        \n",
    "        #num_of_layer = int((len(actor_grads) + 1)/2)\n",
    "        #for layer in range(0, num_of_layer): # for 11 layers\n",
    "            #print('max gradient of layer={}, kernel={}, bias={}'.format( \\\n",
    "            #layer, actor_grads[layer].numpy().max(), actor_grads[layer*2+1].numpy().max()))\n",
    "        \n",
    "        # update actor \n",
    "        self.actor_opt.apply_gradients(zip(actor_grads, self.actor_network.trainable_weights))\n",
    "        \n",
    "\n",
    "           \n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        save trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        path = os.path.join('model', '_'.join([\"MCPG\", \"PM\"]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path) # create a new dir\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor_network)\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        load trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        path = os.path.join('model', '_'.join([\"MCPG\", \"PM\"]))\n",
    "        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor_network)\n",
    "    \n",
    "\n",
    "    def get_cnn_actor_model(self, inputs_shape, model_name, layers=None):\n",
    "        \n",
    "        # self defined initialization\n",
    "        stock_num = inputs_shape[1]\n",
    "        his_window = inputs_shape[2]\n",
    "        feature_num = inputs_shape[3]\n",
    "        if layers is None:\n",
    "            layers = [feature_num]\n",
    "        layers.append(40)\n",
    "        W_init = tl.initializers.truncated_normal(stddev=1e-1)\n",
    "        W_init2 = tl.initializers.truncated_normal(stddev=1e-1)\n",
    "        b_init2 = tl.initializers.constant(value=5e-13)\n",
    "\n",
    "        # build network\n",
    "        ni = Input(inputs_shape)\n",
    "        na = Input((None, stock_num))\n",
    "        nb = Input((None, 1))\n",
    "        na = Reshape((-1, stock_num, 1, 1))(na)\n",
    "        # feature_num -> num\n",
    "        nn = Conv2d(4, (1, 3), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, b_init=None, name='conv1')(ni) #fully connected\n",
    "        # nn = MaxPool2d((3, 3), (2, 2), padding='SAME', name='pool1')(nn)\n",
    "        print(nn.shape)\n",
    "        for i, layer in enumerate(layers[:-1]):\n",
    "            nn = Conv2d(layer, (1, 3), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init2, b_init=None, name='conv'+str(i+2))(nn)\n",
    "            print(nn.shape)\n",
    "        # nn = MaxPool2d((3, 3), (2, 2), padding='SAME', name='pool1')(nn)\n",
    "        \n",
    "        nn = Conv2d(layers[-1], (1, his_window), (1, 1), padding='VALID', act=tf.nn.relu, W_init=W_init2, b_init=None, name='conv'+str(len(layers)+1))(nn)\n",
    "        print(nn.shape)\n",
    "        print(na.shape)\n",
    "        nn = Concat(-1)([nn, na])\n",
    "        print(nn.shape)\n",
    "        nn = Conv2d(1, (1, 1), (1, 1), padding='VALID', act=tf.nn.relu, W_init=W_init2, b_init=None, name='conv'+str(len(layers)+2))(nn)\n",
    "        print(nn.shape)\n",
    "        nn = Flatten()(nn)\n",
    "        print(nn.shape)\n",
    "        nn = Dropout(keep=0.7)(nn)\n",
    "        #nn = Dense(stock_num , act=tf.nn.relu, name='dense1relu')(nn) #  W_init=W_init2, b_init=b_init2,\n",
    "        #nn = BatchNorm()(nn)\n",
    "        #nn = Dense(32, act=tf.nn.relu, name='dense2relu')(nn) # W_init=W_init2, b_init=b_init2,\n",
    "        #nn = BatchNorm()(nn)\n",
    "        nb = Dense(1,  name='output')(nb) # W_init=W_init2,\n",
    "        nn = Concat(-1)([nb, nn])\n",
    "        nn = Lambda(tf.keras.layers.Softmax(-1))(nn)\n",
    "        M = Model(inputs=[ni, na, nb], outputs=nn) # , name=model_name\n",
    "        return M\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad06b7-f2b7-422e-aa9a-1fea39a46c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c845b3-0c29-4393-82dd-64a9b7f567ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd599240-2d31-43e6-82a9-e74085006ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63314d77-b447-448a-b438-0bfa2950d633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e00781-9e73-40e1-91a3-fe75cd079365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d10eb1-1ea4-464c-ac3e-13a3fe01ed54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56079e-99eb-4914-aa9a-7f6712fedfa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ed4e6-bf89-4651-bf33-b8f773a7be93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf598a6-bde9-4a87-8277-f6f794abfd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64944fce-c63a-43c9-b6ca-fe50ff19ff26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
